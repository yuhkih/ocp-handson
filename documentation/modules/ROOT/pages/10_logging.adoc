= OpenShift ログ集約
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:

== 演習の概要
このラボでは、OpenShiftのログ集約機能を触ってみます。

OpenShift の非常に重要な機能の一つに、実行中の環境とアプリケーションPodからログを収集して集約する機能があります。
OpenShift には柔軟性のある *EFK* によるログ集約ソリューションが同梱されています。
*EFK* は、それぞれのソリューションの頭文字を集めたものです。( *E* lasticSearch、*F* luentd、*K* ibana)

クラスタの Logging コンポーネントは、Elasticsearch、Fluentd、Kibana（EFK）をベースにしています。
コレクターである Fluentd は、OpenShift クラスタ内の各ノードにデプロイされています。
これはすべてのノードとコンテナのログを収集し、Elasticsearch（ES）に書き込みます。
Kibana は一元化された Web UI で、ユーザーや管理者は集約されたデータを使ってリッチなビジュアライゼーションやダッシュボードを作成することができます。
管理者は、すべてのログを検索することができます。
アプリケーションの所有者や開発者は、プロジェクトに属するログへのアクセスを許可することができます。
EFK スタックは OpenShift の上で動作します。

[Warning]
====
このラボでは、infra-nodes ラボを完了していることが必要です。
Logging スタックはそのラボで作成された `infra` ノードにインストールされます。
====

[Note]
====
詳細は、以下にあるOpenShiftの公式ドキュメントサイトに記載されています。:
 https://docs.openshift.com/container-platform/4.10/logging/cluster-logging.html
====

[Note]
====
この演習は、ほぼすべて OpenShift の Web コンソールを使用して行われます。
ウェブコンソールとのやりとりはすべて、事実上バックグラウンドで API オブジェクトを作成または操作しています。
プロセスを完全に自動化したり、CLIや他のツールを使用して行うことも可能ですが、これらの方法は現時点では、この演習やドキュメントではカバーされていません。
====

---

### OpenShift Logging をデプロイする

OpenShift Container Platform Cluster Logging は、デフォルト構成で使用するように設計されており、中小規模の OpenShift Container Platform クラスタ向けに調整されています。
以降のインストール手順には、サンプルの Cluster Logging Custom Resource（CR）が含まれており、これを使用して Cluster Logging インスタンスを作成し、Cluster Logging の導入を構成することができます。

デフォルトの Cluster Logging インストールを使用する場合は、サンプルCRを直接使用できます。

配置をカスタマイズしたい場合は、必要に応じてサンプル CR に変更を加えます。
以下では、Cluster Logging インスタンスのインストール時に行うことができる構成、またはインストール後に変更することができる構成について説明します。
Cluster Logging Custom Resource の外でできる変更を含め、各コンポーネントでの作業の詳細については、「構成」のセクションを参照してください。

#### `openshift-logging` namespace を作成する

OpenShift Logging は、独自の名前空間 `openshift-logging` 内で実行されます。
この名前空間はデフォルトでは存在せず、Logging をインストールする前に作成する必要があります。
名前空間は yaml 形式で以下のように表されます。:

[source,yaml]
.openshift_logging_namespace.yaml
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-logging
  annotations:
    openshift.io/node-selector: ""
  labels:
    openshift.io/cluster-logging: "true"
    openshift.io/cluster-monitoring: "true"
----

ネームスペースを作成するには、以下のコマンドを実行します。:

[source,bash,role="execute"]
----
oc create -f {{ HOME_PATH }}/support/openshift_logging_namespace.yaml
----

ネームスペースが作成されたか確認します。

[source,bash,role="execute"]
----
oc get ns openshift-logging
----

次のような結果が表示されるはずです。

```
NAME                STATUS   AGE
openshift-logging   Active   11s
```


#### `Elasticsearch` と `Cluster Logging` Operator をクラスターにインストールする

`EFK` スタックをクラスタにインストールして設定するには、追加の Operator をインストールする必要があります。
これらは、クラスタ内から `Operator Hub` から GUI を介してインストールすることができます。

OpenShift で Operator を使用する際には、Operator を構成するいくつかの基本的な原理を理解しておくことが重要です。
`CustomResourceDefinion (CRD)` と `CustomResource (CR)` は、Kubernetes オブジェクトであり、簡単に説明すると以下の通りです。
`CRD` は、ジェネリックな事前定義された、データの構造体です。
Operator は、`CRD` で定義されたデータをどのように適用するかを理解します。
プログラミング的には、`CRD` はクラスに似ていると考えることができます。
`CustomResource (CR)` は、構造化されたデータが実際の値を持つ `CRD` の実際の実装です。
これらの値は、Operator がサービスを設定するときに使用するものです。
繰り返しになりますが、プログラミング用語では、`CR` はクラスのインスタンス化されたオブジェクトに似ています。

Operator を使用するための一般的なパターンは、まず Operator をインストールし、必要な `CRD` を作成します。
`CRD` が作成された後、どのように動作するか、何をインストールするか、何を設定するかを Operator に伝える `CR` を作成します。
openshift-logging をインストールするには、このパターンに従います。

まず、OpenShift ClusterのGUIにログインします。
`{{ MASTER_URL }}`

その後、以下の手順に従ってください。:

1. Elasticsearch Operator のインストール:
  a. OpenShift コンソールから、 `Operators` → `OperatorHub` をクリックします。
  b. 検索フィールドに `Elasticsearch Operator` と入力し、使用可能なオペレーターのリストから `OpenShift Elasticsearch Operator` を選択し、 `Install` をクリックします。
  c. `Create Operator Subscription` ページで、Update Channelにて *Stable* を選択し、他のすべてのデフォルト設定をそのままに、`Subscribe` をクリックします。
+
これにより、この OpenShift Container Platform クラスタを使用するすべてのユーザーとプロジェクトがこの Operator を利用できるようになります。

2. Cluster Logging Operator のインストール:
+
[Note]
====
`Cluster Logging` Operator を  `openshift-logging` ネームスペースにインストールする必要があります。
`openshift-logging` ネームスペースが前の手順で作成されたことを確認してください。
====

  a. OpenShift コンソールで、`Operators` → `OperatorHub` をクリックします。
  b. 検索フィールドに `OpenShift Logging` と入力し、使用可能なオペレーターのリストから `Red Hat OpenShift Logging` を選択し、 `Install` をクリックします。
  c. `Create Operator Subscription` ページで、`Update Channel` にて *Stable* を選択します。また `Installation Mode` で、*A specific namespace on the cluster* が選択されていることを確認し、`Installed Namespace` にて *openshift-logging* が設定されていることを確認します。他のすべてのデフォルト設定をそのままに、`Subscribe` をクリックします。

3. Operator のインストールを確認する。:

  a. `Operators` → `Installed Operators` のページに切り替えます。

  b. 画面上部のプロジェクト選択にて `Show default projects` のトグルをONにした後、 `openshift-logging` プロジェクトを選択します。

  c. _Status_ 列で、緑色のチェックで、 `InstallSucceeded` もしくは `Copied` そして _Up to date_ のテキストが見えるはずです。
+
[Note]
====
インストール中に Operator が `Failed` ステータスを表示することがあります。
Operator が  `InstallSucceeded` メッセージを表示してインストールが完了した場合、`Failed` メッセージを無視しても問題ありません。
====

4. トラブルシューティング (オプショナル)
+
どちらかの Operator がインストールされているように表示されない場合は、さらにトラブルシューティングを行います。:
+
* `Installed Operators` ページで該当のOperatorを選択し、`Subscription` のタブで、ステータスの下に障害やエラーがないかどうかを確認します。
+
* `Workloads` → `Pods` のページに切り替えて、`openshift-logging` と `openshift-operators` プロジェクトで問題を報告している任意の `Pod` のログを確認します。


#### Logging `CustomResource (CR)` インスタンスを作成する

Operator を `CRD` と一緒にインストールしたので、Logging `CR` を作成して、Logging のインストールを開始します。
これは、Logging をインストールして設定する方法を定義します。

1. OpenShift Consoleで、`Administration` → `Custom Resource Definitions` ページに切り替えます。

2. `Custom Resource Definitions` のページで、 `ClusterLogging` をクリックします。

3. `Custom Resource Definition Overview` ページで、`Actions` メニューから `View Instances` を選択する。
+
[Note]
====
`404` のエラーが表示されても、慌てないでください。
Operator のインストールは成功したものの、Operator 自体のインストールが完了しておらず、 `CustomResourceDefinition` がまだ作成されていない可能性があります。
しばらく待ってからページを更新してください。
====
+
4. `Cluster Loggings` ページで、 `Create Cluster Logging` をクリックします。
+
5. `YAML` エディタで、コードを以下で置き換えます。:

[source,yaml]
.openshift_logging_cr.yaml
----
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"
  namespace: "openshift-logging"
spec:
  managementState: "Managed"
  logStore:
    type: "elasticsearch"
    elasticsearch:
      nodeCount: 3
      storage:
         storageClassName: gp2
         size: 100Gi
      redundancyPolicy: "SingleRedundancy"
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      resources:
        request:
          memory: 4G
  visualization:
    type: "kibana"
    kibana:
      replicas: 1
      nodeSelector:
        node-role.kubernetes.io/infra: ""
  curation:
    type: "curator"
    curator:
      schedule: "30 3 * * *"
      nodeSelector:
        node-role.kubernetes.io/infra: ""
  collection:
    logs:
      type: "fluentd"
      fluentd: {}
      nodeSelector:
        node-role.kubernetes.io/infra: ""
----

そして `Create` をクリックします。

#### Logging インストールを確認する

Logging が作成されたので、動作しているかどうかを確認してみましょう。

1. `Workloads` → `Pods` ページに移動します。

2. `openshift-logging` プロジェクトを選択します。

クラスタ Logging （Operator 自身）、Elasticsearch、Fluentd、Kibana のPodが表示されているはずです。

または、次のコマンドを使用してコマンドラインから検証することもできます。:

[source,bash,role="execute"]
----
oc get pods -n openshift-logging
----

最終的には、次のようなものが表示されるはずです。:

----
NAME                                            READY   STATUS    RESTARTS   AGE
cluster-logging-operator-5d4b6f7b99-ksr5s       1/1     Running   0          113s
collector-2p5fx                                 2/2     Running   0          26s
collector-7lw5r                                 2/2     Running   0          42s
collector-8stvf                                 2/2     Running   0          32s
collector-b7qs8                                 2/2     Running   0          27s
collector-clfsc                                 2/2     Running   0          16s
collector-f2tzf                                 2/2     Running   0          31s
collector-j6hxp                                 2/2     Running   0          10s
collector-kdvj8                                 2/2     Running   0          30s
collector-q6wck                                 2/2     Running   0          21s
collector-sgndk                                 2/2     Running   0          17s
collector-w5ds9                                 2/2     Running   0          29s
collector-zswpb                                 2/2     Running   0          34s
elasticsearch-cdm-mnc985r3-1-5c45b9bd9f-4nx56   2/2     Running   0          70s
elasticsearch-cdm-mnc985r3-2-779989b7bb-z9dpp   1/2     Running   0          69s
elasticsearch-cdm-mnc985r3-3-6d754c8cbf-fx8wd   1/2     Running   0          68s
kibana-655877db88-njsqq                         2/2     Running   0          70s
----

_collector_ *Pods* は、 *DaemonSet* としてデプロイされます。*DaemonSet* は、特定の *Pods* が、クラスタ内の特定の *Nodes* で常に実行されるための仕組みです。:


[source,bash,role="execute"]
----
oc get daemonset -n openshift-logging
----

以下のようなものを見ることができます。:

----
NAME        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
collector   10        10        10      10           10          kubernetes.io/os=linux   2m55s
----

クラスタ内の *Node* ごとに1つの `collector` *Pod* が必要です。
*Master* も *Node* であり、`collector` はそこでも様々なログを読み取るために実行されることを覚えておいてください。

また、ElasticSearch 用のストレージが自動的にプロビジョニングされていることがわかります。
このプロジェクトの *PersistentVolumeClaim* オブジェクトにクエリを実行すると、新しいストレージが表示されます。

[source,bash,role="execute"]
----
oc get pvc -n openshift-logging
----

以下のようなものが見えるはずです。:

----
NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS
MODES   STORAGECLASS                  AGE
elasticsearch-elasticsearch-cdm-ggzilasv-1   Bound    pvc-f3239564-389c-11ea-bab2-06ca7918708a   100Gi      RWO
        ocs-storagecluster-ceph-rbd   15m
elasticsearch-elasticsearch-cdm-ggzilasv-2   Bound    pvc-f324a252-389c-11ea-bab2-06ca7918708a   100Gi      RWO
        ocs-storagecluster-ceph-rbd   15m
elasticsearch-elasticsearch-cdm-ggzilasv-3   Bound    pvc-f326aa7d-389c-11ea-bab2-06ca7918708a   100Gi      RWO
        ocs-storagecluster-ceph-rbd   15m
----		

[Note]
====
Metrics ソリューションの場合と同様に、Logging 構成( `CR` )で適切な `NodeSelector` を定義して、Logging コンポーネントが infra ノードにしかデプロイされないようにしています。
つまり、`DaemonSet` は FluentD が *すべての* ノードで実行されることを保証しています。
そうでなければ、すべてのコンテナログをキャプチャすることはできません。
====

#### _Kibana_ にアクセスする

前述の通り、_Kibana_ はフロントエンドであり、ユーザーや管理者が OpenShift Logging スタックにアクセスするためのインターフェイスです。
_Kibana_ ユーザーインターフェースにアクセスするには、まず Kibana の *Service* を公開するために設定された *Route* を見て、そのパブリックアクセス URL を調べます。:

_Kibana_ route を見つけてアクセスするには:

1. OpenShift console から、 `Networking` → `Routes` ページをクリックします。

2. `openshift-logging` プロジェクトを選択します。

3. `Kibana` route をクリックします。

4. `Location` フィールドで、表示されている URL をクリックします。

5.  SSL 証明書をアクセプトします。

あるいは、コマンドラインから取得することもできます。:

[source,bash,role="execute"]
----
oc get route -n openshift-logging
----

以下のようなものが見えるはずです。:

----
NAME     HOST/PORT                                                           PATH   SERVICES   PORT    TERMINATION          WILDCARD
kibana   kibana-openshift-logging.{{ ROUTE_SUBDOMAIN }}          kibana     <all>   reencrypt/Redirect   None
----

または、control+click  をクリックすることができます。:

https://kibana-openshift-logging.{{ ROUTE_SUBDOMAIN }}

EFK インストールの一部として設定されている特別な認証プロキシがあり、その結果、Kibana はアクセスに OpenShift の資格情報を必要とします。

OpenShift Console に cluster-admin ユーザーとして認証済みのため、Kibana の管理画面が表示されます。

#### インデックスパターンの設定

Kibanaを開いたら、ログを表示する前に、 KibanaがElasticSearchにクエリを実行するために使用する `index pattern` を定義する必要があります。

1. 次の画面で、下図のようにインデックスパターンに `app*` と入力し、 `Next Step` をクリックします。
+
image::images/logging-kibana-indexpattern.png[]
+
2. 次の画面で、以下に示すように、ドロップダウンボックスで `@timestamp` を選択します。
+
image::images/logging-kibana-indexpattern-timestamp.png[]
+
3. `Create Index Pattern` をクリックします。
4. 以下の概要画面が表示されます。
+
image::images/kibana-summary-ip.png[]
+
5. 画面左上の `Discover` をクリックします

#### _Kibana_ を使ってクエリを行う

_Kibana_ の Web インターフェースが立ち上がったら、クエリを実行できるようになります。
_Kibana_ は、クラスタから送られてくるすべてのログを問い合わせるための強力なインターフェイスをユーザに提供します。

デフォルトでは、_Kibana_ は過去15分以内に受信したすべてのログを表示します。
この時間間隔は右上で変更できます。
ログメッセージはページの中央に表示されます。
受信したすべてのログメッセージは、ログメッセージの内容に基づいてインデックス化されます。
各メッセージには、そのログメッセージに関連付けられたフィールドがあります。
個々のメッセージを構成するフィールドを見るには、ページの中央にある各メッセージの側面にある矢印をクリックします。
これにより、含まれているメッセージ フィールドが表示されます。

メッセージに表示するフィールドを選択するには、左側の `Available Fields` ラベルの手前を見てください。
その下には選択可能なフィールドがあり、画面の中央に表示されます。
利用可能なフィールド `Available Fields` の下にある `hostname` フィールドを見つけて、 `add` をクリックします。
これで、メッセージペインに各メッセージのホスト名が表示されることに気づくでしょう。
これ以外にもフィールドを追加することができます。 `kubernetes.pod_name` と `message` の `add` ボタンをクリックします。

ログに対するクエリを作成するには、検索ボックスの右下にある `Add a filter +` リンクを使用することができます。
これにより、メッセージのフィールドを使ってクエリを作成することができます。
例えば、 `lab-ocp-cns` namespace のすべてのログメッセージを見たい場合、以下のようにします。:

1. `Add a filter +` をクリックします。

2. `Fields` インプットボックスで、 `kubernetes.namespace_name` とタイプします。
クエリをビルドするための全ての可能なフィールドがある事に注目してください。

3. 次に、 `is` を選択します。

4. `Value` フィールドで、 `lab-ocp-cns` とタイプします。

5. "Save" ボタンをクリックします。

画面の中央には `lab-ocp-cns` namespace にあるすべてのPodからのログが表示されているはずです。

もちろん、さらにフィルタを追加してクエリを絞り込むこともできます。

Kibanaでは、クエリを保存して後で使えるようにすることができます。クエリを保存するには、以下のようにします。:

1. 画面上部の `Save` をクリックします。

2. 保存したい名前を入力します。ここでは、`lab-ocp-cns Namespace` と入力します。

一度保存しておけば、後で `Open` ボタンを押してこのクエリを選択することで利用することができます。

時間をかけて _Kibana_ のページを探索し、より多くのクエリを追加したり実行したりして経験を積んでください。
これは本番環境のクラスタを使用する際に役立つでしょう。
探しているログをこのコンソールから取得することができるようになります。



### ログを外部システムに転送する

このセクションでは、ログを外部ログシステムに転送する方法を示します。

`ClusterLogForwarder` によって指定された新しい `CustomResourceDefinition（CRD）` は、ログを外部（または内部）システムに転送するために内部のFluentd `configmas` を作成または変更するために使用されます。Cluster Logging Operatorクラスタに存在できる `ClusterLogForwarder` は1つだけであり、すべてのログ転送ルールが組み合わされています。

外部のサードパーティシステムにクラスタログを転送するには、 `ClusterLogForwarder` カスタムリソース（CR）で指定された出力とパイプラインを組み合わせて、OpenShift Container Platformクラスタの内部および外部の特定のエンドポイントにログを送信することが必要です。また、 `inputs` を使用して、特定のプロジェクトに関連するアプリケーションログをエンドポイントに転送することができます。これらの概念について詳しく学びましょう。

* `output` は、定義したログデータの宛先、またはログの送信先です。`output` の種類は以下の通りです。
** `elasticsearch` : 外部のElasticsearch v5.xまたはv6.xインスタンス。Elasticsearchの出力はTLS接続を使用できます。
** `fluentdForward` : Fluentdをサポートする外部のログアグリゲーションソリューションです。このオプションはFluentdの転送プロトコルを使用します。`fluentForward` outputはTCPまたはTLS接続を使用でき、秘密鍵のshared_keyフィールドを提供することで *共有鍵認証* をサポートします。共有鍵認証は、TLS の有無にかかわらず使用できます。 
** `syslog` : syslogRFC3164またはRFC5424プロトコルをサポートする外部ログ集約ソリューションです。syslog出力は、UDP、TCP、またはTLS接続を使用できます。
** `kafka` : Kafkaブローカーです。`kafka` outputは、TCPまたはTLS接続を使用できます。
** `default` : 内部の OpenShift Container Platform Elasticsearch インスタンスです。デフォルトのoutputを設定する必要はありません。デフォルトのoutputを設定した場合、デフォルトのoutputはクラスターロギングオペレーター用に予約されているため、エラーメッセージが表示されます。

output URL スキームが TLS (HTTPS、TLS、または UDPS) を必要とする場合、TLS サーバーサイド認証が有効になります。クライアント認証も有効にするには、output に `openshift-logging` プロジェクト内の secret を指定する必要があります。この secret には、それぞれの証明書を指す *tls.crt*、*tls.key*、および *ca-bundle.crt* というキーが必要です。

* `pipeline` は、1つのログタイプから1つまたは複数の出力への単純なルーティング、またはどのログを送信するかを定義します。ログタイプは以下のいずれかです。
** `application` : インフラストラクチャコンテナアプリケーションを除く、クラスタで実行されているユーザーアプリケーションによって生成されたコンテナログ。
** `infrastructure` : openshift *、kube *、またはデフォルトのプロジェクトで実行されるPodからのコンテナーログと、ノードファイルシステムから供給されるジャーナルログ。
** `audit` : ノードの監査システム（auditd）が生成するログと、Kubernetes APIサーバーおよびOpenShift APIサーバーの監査ログ。

パイプラインのキーと値のペアを使用して、送信ログメッセージにラベルを追加することができます。たとえば、他のデータセンターに転送されるメッセージにラベルを追加したり、タイプ別にログにラベルを付けることができます。オブジェクトに追加されたラベルは、ログメッセージと一緒に転送されます。

* inputは、特定のプロジェクトに関連付けられたアプリケーションログをパイプラインに転送します。

詳細については、
link:https://docs.openshift.com/container-platform/4.10/logging/cluster-logging-external.html[OpenShiftの公式ドキュメントサイト]をご覧ください。

#### 外部Syslogサーバーへのログの送信

ここでは簡略化のため、コンテナ化したSyslogサーバーを `external-logs` という名前空間に配置し、外部のSyslogサーバーをエミュレートすることにします。

アプリケーションログとインフラログを分離する方法も紹介したいので、2つの（コンテナ化した）外部Syslogをデプロイします。1つは転送されたアプリケーションログを受信するため、もう1つは転送されたインフラログを受信するためです。

まず、`external-logs` というネームスペースを作成し、そこにSyslogサーバを配置します。

[source,bash,role="execute"]
----
oc new-project external-logs
----

では、そのネームスペースに `Syslog` サーバをデプロイしてみましょう。そのために、必要なリソースをすべて含むYAMLファイルを使用します。

[source,bash,role="execute"]
----
oc create -f /opt/app-root/src/support/extlogs-syslog.yaml -n external-logs
----

すべてが正常に機能していることを確認しましょう。外部レジストリ用にイメージがプルされるまで1分かかる場合があります。すべてがOKの場合、次のような出力が得られるはずです。

[source,bash,role="execute"]
----
oc get pods -n external-logs
----

次の出力が表示されます。

----
NAME                               READY   STATUS    RESTARTS   AGE
syslog-ng-84c59fdc8-mdwrs          1/1     Running   0          81s
syslog-ng-infra-697fc7597f-gwrxd   1/1     Running   0          81s
----

いずれかのPodが `CrashLoopBackOff` 状態になっている場合は、`oc delete pods --all -n external-logs` を実行してPodを再起動してください。

外部 Syslog サーバが利用可能になったので、`ClusterLogForwarder` を作成してログ転送ルールを設定しましょう。まず、YAMLファイルを見てみましょう。

----
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging
spec:
  outputs: (1)
  - name: rsyslog-app
    syslog:
      facility: user
      payloadKey: message
      rfc: RFC3164
      severity: informational
    type: syslog (2)
    url: udp://syslog-ng.external-logs.svc:514 (3)
  - name: rsyslog-infra
    syslog:
      facility: user
      payloadKey: message
      rfc: RFC3164
      severity: informational
    type: syslog
    url: udp://syslog-ng-infra.external-logs.svc:514 (4)
  pipelines: (5)
  - inputRefs: (6)
    - application (7)
    labels:
      syslog: app
    name: syslog-app
    outputRefs:
    - rsyslog-app (8)
    - default
  - inputRefs:
    - infrastructure (8)
    labels:
      syslog: infra
    name: syslog-infra
    outputRefs:
    - rsyslog-infra (9)
    - default
----

このYAMLファイルには、いくつかの注目すべきフィールドがあります。

* (1) `outputs` セクションは、すべてのリモートログシステムを定義します。この例では、2つの Syslog サーバーがあります。
* (2) 使用されているログアグリゲータの種類を定義します。
* (3) アプリケーション関連のログを保存するためのURLです。 `external-logs` ネームスペースにあるサービスを指しています。
* (4) インフラ関連のログを保存するためのURLです。`external-logs` ネームスペースにあるサービスを指しています。
* (5）`pipeline` は、先に定義したアウトプットに送るべきログのソースと性質を定義しています。
* (6) `inputRefs` は送信するログの性質を記述するためのもので、注意点として、アプリケーション、インフラ、OpenShift の監査ログ (API アクセスなど) のための監査のいずれかを指定できます。
* 2つのinputRefがあり、(7)はアプリケーションログ用、(8)はインフラストラクチャログ用です。

各 `inputRefs` セクションには、ログがどこに送られるかを示す `outRefs` が含まれており、`spec` セクションの最初に定義された `outputs` (1) を参照しています。

では、YAML ファイルを使用して `ClusterLogForwarder` リソースを作成してみましょう。

[source,bash,role="execute"]
----
oc create -f /opt/app-root/src/support/extlogs-clusterlogforwarder.yaml
----

CRが作成されると、Cluster Logging Operatorは `Collector` Podsをデプロイします。デプロイされるのを待ちます。

[source,bash,role="execute"]
----
oc rollout status ds/collector -n openshift-logging
----

Podが再展開されない場合は、 `Collector` Podを手動で削除して、強制的に再展開させることができます。

[source,bash,role="execute"]
----
oc delete pod --selector logging-infra=collector -n openshift-logging
----

すべての `Collector` Podが Running 状態になったことを確認しましょう。

[source,bash,role="execute"]
----
oc get pod --selector logging-infra=collector -n openshift-logging
----

このようなものが出力されるはずです。

----
NAME              READY   STATUS    RESTARTS   AGE
collector-2mk4h   2/2     Running   0          37s
collector-4dfnc   2/2     Running   0          38s
collector-99rh4   2/2     Running   0          37s
collector-c7msc   2/2     Running   0          38s
collector-gb7nh   2/2     Running   0          38s
collector-k8khn   2/2     Running   0          37s
collector-lt8j4   2/2     Running   0          38s
collector-pzqxw   2/2     Running   0          37s
collector-w54c5   2/2     Running   0          37s
----

ここで、2台のSyslogサーバにログが転送されていることを確認しましょう。Syslogサーバーはコンテナ内の `/var/log/messages` ファイルにログを保存していますので、Webコンソールからコンテナに`oc exec` して内容を確認する必要があります。

今回はOpenShift Console Terminalを使用してPodにアクセスし、 `/var/log/messages` の内容を確認します。

1. Administrator Viewを開き、 `workloads→Pods` と進みます。 `external-logs` Projectにいることを確認します。
+
image::images/logging-syslog-pods.png[Syslog Pods]
+
2. `syslog-ng-infra-xyz` のような名前の `syslog-infra` Podをクリックし、`Terminal` タブに移動します (# プロンプトを表示するには、何度かエンターキーを押す必要があるかもしれません)。
+
image::images/logging-syslog-terminal-infra.png[Syslog Terminal]
+
3. ターミナルボックスに、`tail -f /var/log/messages` と入力します。すると、転送されたログがターミナルに表示されるはずです。
+
image::images/logging-syslog-logs.png[Syslog logs]


これで完了です！この手順をもう一方のPodで繰り返して、アプリケーション・ログも正しく転送されることを確認できます。
